import os
import re
import logging
import asyncio
import json
import random
import time
import requests
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from datetime import datetime
from fake_useragent import UserAgent
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    NoSuchElementException, TimeoutException,
    StaleElementReferenceException, InvalidSelectorException, WebDriverException
)
from webdriver_manager.chrome import ChromeDriverManager
from telegram import Bot
from telegram.constants import ParseMode
from telegram.error import TelegramError

# --- Configura√ß√£o de Logging ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(name)s:%(funcName)s:%(lineno)d] - %(message)s",
    handlers=[logging.StreamHandler()]
)
logging.getLogger("webdriver_manager").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("telegram.bot").setLevel(logging.WARNING)
logging.getLogger("telegram.ext").setLevel(logging.WARNING)
logging.getLogger("bs4").setLevel(logging.WARNING)

logger = logging.getLogger("SCRAPER_USADOS_GERAL")

# --- Configura√ß√µes do Scraper ---
SELETOR_ITEM_PRODUTO_USADO = "div.s-result-item.s-asin"
# XPath ATUALIZADO para melhor identificar itens com ofertas de usado
SELETOR_INDICADOR_USADO_XPATH = (
    ".//span[contains(translate(., 'OFERTA DE PRODUTO USADO', 'oferta de produto usado'), 'oferta de produto usado') or "
    "contains(translate(., 'OFERTAS DE PRODUTOS USADOS', 'ofertas de produtos usados'), 'ofertas de produtos usados') or "
    "contains(translate(., 'USADO COMO NOVO', 'usado como novo'), 'usado como novo') or "
    "(ancestor::div[@data-cy='secondary-offer-recipe'] and (contains(translate(., 'USADO', 'usado'), 'usado') or contains(translate(., 'USADA', 'usada'), 'usada')) ) or " # Para "X oferta de produto usado"
    "(.//div[contains(@class, 's-price-instructions-style')]//a//span[contains(translate(., 'USADO', 'usado'), 'usado')])" # Mant√©m a l√≥gica original como fallback
    "]"
)
logger.info(f"Usando SELETOR_INDICADOR_USADO_XPATH: {SELETOR_INDICADOR_USADO_XPATH}")
SELETOR_RESULTADOS_CONT = "div.s-main-slot.s-result-list.s-search-results.sg-row"

URL_GERAL_USADOS_BASE = (
    "https://www.amazon.com.br/s?i=warehouse-deals&srs=24669725011&bbn=24669725011"
    "&rh=n%3A24669725011&s=popularity-rank&fs=true&xpid=71AiW8sVquI1l"
)
NOME_FLUXO_GERAL = "Amazon Quase Novo (Geral)"

MIN_DESCONTO_USADOS_STR = os.getenv("MIN_DESCONTO_PERCENTUAL_USADOS", "40").strip()
try:
    MIN_DESCONTO_USADOS = int(MIN_DESCONTO_USADOS_STR)
    if not (0 <= MIN_DESCONTO_USADOS <= 100):
        logger.warning(f"MIN_DESCONTO_USADOS ({MIN_DESCONTO_USADOS}%) fora do intervalo. Usando 40%.")
        MIN_DESCONTO_USADOS = 40
except ValueError:
    logger.warning(f"Valor inv√°lido para MIN_DESCONTO_PERCENTUAL_USADOS ('{MIN_DESCONTO_USADOS_STR}'). Usando 40%.")
    MIN_DESCONTO_USADOS = 40
logger.info(f"Desconto m√≠nimo para notifica√ß√£o de usados: {MIN_DESCONTO_USADOS}% (Observa√ß√£o: este filtro n√£o est√° sendo aplicado explicitamente no c√≥digo atual antes da notifica√ß√£o)")


USAR_HISTORICO_STR = os.getenv("USAR_HISTORICO_USADOS", "true").strip().lower()
USAR_HISTORICO = USAR_HISTORICO_STR == "true"
logger.info(f"Usar hist√≥rico para produtos usados: {USAR_HISTORICO}")

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN", "").strip()
TELEGRAM_CHAT_IDS_STR = os.getenv("TELEGRAM_CHAT_ID", "").strip()
TELEGRAM_CHAT_IDS_LIST = [chat_id.strip() for chat_id in TELEGRAM_CHAT_IDS_STR.split(',') if chat_id.strip()]

MAX_PAGINAS_POR_LINK_GLOBAL = int(os.getenv("MAX_PAGINAS_USADOS_GERAL", "500"))
logger.info(f"M√°ximo de p√°ginas para busca geral de usados: {MAX_PAGINAS_POR_LINK_GLOBAL}")

HISTORY_DIR_BASE = "history_files_usados"
DEBUG_LOGS_DIR_BASE = "debug_logs_usados"
HISTORY_FILENAME_USADOS_GERAL = "price_history_USADOS_GERAL.json"

os.makedirs(HISTORY_DIR_BASE, exist_ok=True)
logger.info(f"Diret√≥rio de hist√≥rico '{HISTORY_DIR_BASE}' verificado/criado.")
os.makedirs(DEBUG_LOGS_DIR_BASE, exist_ok=True)
logger.info(f"Diret√≥rio de logs de debug '{DEBUG_LOGS_DIR_BASE}' verificado/criado.")

bot_instance_global = None
if TELEGRAM_TOKEN and TELEGRAM_CHAT_IDS_LIST:
    try:
        bot_instance_global = Bot(token=TELEGRAM_TOKEN)
        logger.info(f"Inst√¢ncia global do Bot Telegram criada. IDs de Chat: {TELEGRAM_CHAT_IDS_LIST}")
    except Exception as e:
        logger.error(f"Falha ao inicializar Bot global: {e}", exc_info=True)
else:
    logger.warning("Token do Telegram ou Chat IDs n√£o configurados. Notifica√ß√µes Telegram desabilitadas.")


async def process_used_products_geral_async(driver, base_url, nome_fluxo, history, logger, max_paginas=MAX_PAGINAS_POR_LINK_GLOBAL):
    logger.info(f"--- Iniciando processamento para: {nome_fluxo} --- URL base: {base_url} ---")
    total_produtos_usados_qualificados = []
    pagina_atual = 1
    max_tentativas_pagina = 3
    consecutive_empty_pages = 0
    max_consecutive_empty_pages = 3

    if max_paginas != MAX_PAGINAS_POR_LINK_GLOBAL:
        logger.info(f"O par√¢metro 'max_paginas' ({max_paginas}) √© diferente de MAX_PAGINAS_POR_LINK_GLOBAL ({MAX_PAGINAS_POR_LINK_GLOBAL}). Usando {max_paginas}.")

    while pagina_atual <= max_paginas:
        url_pagina = get_url_for_page_worker(base_url, pagina_atual, logger)
        logger.info(f"[{nome_fluxo}] Carregando P√°gina: {pagina_atual}/{max_paginas}, URL: {url_pagina}")

        page_processed_successfully = False
        for tentativa in range(1, max_tentativas_pagina + 1):
            logger.info(f"[{nome_fluxo}] Tentativa {tentativa}/{max_tentativas_pagina} de carregar e processar URL: {url_pagina}")
            try:
                await asyncio.to_thread(driver.get, url_pagina)
                await asyncio.sleep(random.uniform(3, 6))
                await asyncio.to_thread(wait_for_page_load, driver, logger)
                await simulate_scroll(driver, logger)

                try:
                    timestamp_page_dump = datetime.now().strftime('%Y%m%d_%H%M%S')
                    page_dump_filename = f"page_dump_p{pagina_atual}_fluxo_{nome_fluxo.replace(' ', '_')}_{timestamp_page_dump}.html"
                    page_dump_path = os.path.join(DEBUG_LOGS_DIR_BASE, page_dump_filename)
                    with open(page_dump_path, "w", encoding="utf-8") as f_html_dump:
                        f_html_dump.write(driver.page_source)
                    logger.info(f"HTML da p√°gina {pagina_atual} salvo em: {page_dump_path}")
                except Exception as e_save_dump:
                    logger.error(f"Erro ao salvar o HTML da p√°gina {pagina_atual}: {e_save_dump}")

                if check_captcha_sync_worker(driver, logger):
                    logger.error(f"[{nome_fluxo}] CAPTCHA detectado na p√°gina {pagina_atual}. Interrompendo fluxo para esta URL base.")
                    return total_produtos_usados_qualificados

                if check_amazon_error_page_sync_worker(driver, logger):
                    logger.error(f"[{nome_fluxo}] P√°gina de erro da Amazon detectada na p√°gina {pagina_atual}.")
                    if tentativa < max_tentativas_pagina:
                        logger.info("Tentando novamente ap√≥s delay...")
                        await asyncio.sleep(random.uniform(10, 20))
                        continue
                    else:
                        logger.error(f"[{nome_fluxo}] Falha ao carregar p√°gina de produtos ap√≥s {max_tentativas_pagina} tentativas devido a p√°gina de erro. Interrompendo.")
                        return total_produtos_usados_qualificados
                
                try:
                    WebDriverWait(driver, 20).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, SELETOR_RESULTADOS_CONT))
                    )
                    logger.info(f"Cont√™iner de resultados '{SELETOR_RESULTADOS_CONT}' encontrado na p√°gina {pagina_atual}.")
                except TimeoutException:
                    logger.warning(f"Cont√™iner de resultados '{SELETOR_RESULTADOS_CONT}' n√£o encontrado na p√°gina {pagina_atual} ap√≥s timeout.")
                    
                items_selenium = driver.find_elements(By.CSS_SELECTOR, SELETOR_ITEM_PRODUTO_USADO)
                logger.info(f"P√°gina {pagina_atual}: Encontrados {len(items_selenium)} elementos com seletor Selenium '{SELETOR_ITEM_PRODUTO_USADO}'.")

                if not items_selenium:
                    logger.info(f"P√°gina {pagina_atual} n√£o cont√©m produtos com o seletor principal. Verificando se √© o fim.")
                    next_button_disabled = False
                    try:
                        driver.find_element(By.CSS_SELECTOR, ".s-pagination-item.s-pagination-next.s-pagination-disabled")
                        logger.info("Bot√£o 'Pr√≥ximo' est√° desabilitado. Fim da pagina√ß√£o.")
                        next_button_disabled = True
                    except NoSuchElementException:
                        logger.debug("Bot√£o 'Pr√≥ximo' n√£o est√° desabilitado ou n√£o foi encontrado.")
                    
                    if next_button_disabled:
                        return total_produtos_usados_qualificados
                    
                    consecutive_empty_pages += 1
                    if consecutive_empty_pages >= max_consecutive_empty_pages:
                        logger.warning(f"{max_consecutive_empty_pages} p√°ginas vazias consecutivas. Considerando fim da busca para {nome_fluxo}.")
                        return total_produtos_usados_qualificados
                    logger.info(f"P√°gina {pagina_atual} vazia, mas n√£o √© o fim. Tentativa {consecutive_empty_pages}/{max_consecutive_empty_pages}.")
                    page_processed_successfully = True 
                    break 

                consecutive_empty_pages = 0 
                produtos_processados_e_notificados_na_pagina = 0

                for idx, item_element_selenium in enumerate(items_selenium, 1):
                    item_logger = logging.getLogger(f"{logger.name}.Item_{pagina_atual}_{idx}")
                    item_logger.debug(f"Processando bloco de item {idx} da p√°gina {pagina_atual}")
                    
                    nome, link, asin, price = None, None, None, None

                    try:
                        # Etapa 1: Verificar se o item √© "usado" usando o XPath no elemento Selenium
                        try:
                            # Usar o SELETOR_INDICADOR_USADO_XPATH atualizado aqui
                            indicador_usado_el = item_element_selenium.find_element(By.XPATH, SELETOR_INDICADOR_USADO_XPATH)
                            item_logger.debug(f"Indicador de 'usado' encontrado via XPath: '{indicador_usado_el.text.strip() if indicador_usado_el.text else 'Indicador presente (sem texto direto no elemento XPath)'}'")
                        except NoSuchElementException:
                            # Se o data-asin estiver dispon√≠vel, logue-o para facilitar a depura√ß√£o
                            data_asin_sel = item_element_selenium.get_attribute('data-asin')
                            item_logger.debug(f"Item (ASIN Sel: {data_asin_sel if data_asin_sel else 'N/A'}) N√ÉO √© uma listagem direta de 'usado' ou n√£o tem oferta de usado clara (XPath '{SELETOR_INDICADOR_USADO_XPATH}' n√£o encontrado). Ignorando este item.")
                            continue

                        item_html = item_element_selenium.get_attribute('outerHTML')
                        item_soup = BeautifulSoup(item_html, 'html.parser')

                        # --- IN√çCIO DA INTEGRA√á√ÉO DOS SNIPPETS FORNECIDOS PELO USU√ÅRIO ---
                        
                        # Snippet de Extra√ß√£o do Nome
                        title_div = item_soup.find('div', {'data-cy': 'title-recipe'})
                        if title_div:
                            h2 = title_div.find('h2')
                            span_nome_tag = h2.find('span') if h2 else None # Renomeado para evitar conflito
                            nome = span_nome_tag.get_text(strip=True) if span_nome_tag else None
                        else:
                            nome = None # Conforme snippet original do usu√°rio

                        if not nome:
                            item_logger.debug("Nome do produto vazio (BS). Ignorando.") # Log do snippet do usu√°rio
                            continue
                        item_logger.debug(f"Nome (BS): '{nome}'") # Log adaptado

                        # Snippet de Extra√ß√£o do Link e ASIN
                        link_tag = item_soup.find('a', href=re.compile(r'/dp/')) # Snippet do usu√°rio
                        if link_tag and link_tag.has_attr('href'):
                            href_val = link_tag['href']
                            link = f"https://www.amazon.com.br{href_val}" if href_val.startswith("/") else href_val
                            item_logger.debug(f"Link (BS): '{link}'") # Log adaptado
                        else:
                            item_logger.warning("Link principal do produto n√£o encontrado. Ignorando item.") # Log do snippet do usu√°rio
                            continue

                        asin_match = re.search(r'/dp/([A-Z0-9]{10})', link) # Snippet do usu√°rio
                        if asin_match:
                            asin = asin_match.group(1)
                            item_logger.debug(f"ASIN (BS): '{asin}'") # Log adaptado
                        else:
                            # Tenta pegar do atributo data-asin do item principal como fallback
                            data_asin_value = item_element_selenium.get_attribute('data-asin')
                            if data_asin_value and len(data_asin_value) == 10:
                                asin = data_asin_value
                                item_logger.debug(f"ASIN (BS, fallback de data-asin): '{asin}'")
                            else:
                                item_logger.warning(f"ASIN n√£o encontrado no link '{link}' nem via data-asin. Ignorando item.") # Log do snippet do usu√°rio, adaptado
                                continue
                        
                        # Snippet de Extra√ß√£o do Pre√ßo
                        price_text_bs = None
                        # Tentativa de pegar o pre√ßo da oferta de usado espec√≠fica primeiro (mais confi√°vel)
                        secondary_offer_div = item_soup.find('div', {'data-cy': 'secondary-offer-recipe'})
                        if secondary_offer_div:
                            span_price_in_secondary = secondary_offer_div.find('span', class_='a-color-base')
                            if span_price_in_secondary:
                                price_text_bs = span_price_in_secondary.get_text(strip=True)
                                item_logger.debug(f"Pre√ßo (BS, via 'secondary-offer-recipe'): '{price_text_bs}'")
                        
                        # Se n√£o encontrou, usa o m√©todo do snippet do usu√°rio (iterar todos os spans)
                        if not price_text_bs:
                            item_logger.debug("Pre√ßo n√£o encontrado em 'secondary-offer-recipe'. Usando itera√ß√£o de spans (snippet do usu√°rio).")
                            for span_tag in item_soup.find_all('span'): # span_tag para n√£o conflitar
                                text = span_tag.get_text(strip=True)
                                if text.startswith('R$'):
                                    price_text_bs = text
                                    item_logger.debug(f"Pre√ßo (BS, via itera√ß√£o de span): '{price_text_bs}'")
                                    break # Conforme snippet do usu√°rio

                        if price_text_bs:
                            match = re.search(r'R\$\s?([\d.,]+)', price_text_bs) # Snippet do usu√°rio
                            if match:
                                cleaned_price_str = match.group(1).replace('.', '').replace(',', '.')
                                try:
                                    price = float(cleaned_price_str)
                                    item_logger.debug(f"Pre√ßo final (BS): {price}") # Log adaptado
                                except ValueError:
                                    item_logger.warning(f"Erro ao converter pre√ßo '{cleaned_price_str}' para float.") # Log do snippet do usu√°rio
                                    continue
                            else:
                                item_logger.warning(f"Formato de pre√ßo inesperado: '{price_text_bs}'. Ignorando item.") # Log do snippet do usu√°rio
                                continue
                        else:
                            item_logger.warning(f"Pre√ßo n√£o encontrado para ASIN {asin}. Ignorando item.") # Log do snippet do usu√°rio
                            continue

                        # --- FIM DA INTEGRA√á√ÉO DOS SNIPPETS ---

                        if not all([nome, asin, link, price is not None]):
                            item_logger.warning(f"Dados incompletos para ASIN {asin if asin else 'desconhecido'} ap√≥s extra√ß√£o BS. Nome: {nome}, Link: {link}, Pre√ßo: {price}. Ignorando.")
                            continue

                        produto = {
                            "nome": nome, "asin": asin, "link": link,
                            "preco_usado": price, "timestamp": datetime.now().isoformat(),
                            "fluxo": nome_fluxo
                        }

                        if USAR_HISTORICO:
                            preco_historico_info = history.get(asin)
                            if preco_historico_info:
                                preco_historico_val = preco_historico_info.get("preco_usado")
                                if preco_historico_val and preco_historico_val <= price:
                                    item_logger.info(f"ASIN {asin}: Pre√ßo atual (R${price:.2f}) n√£o √© menor que hist√≥rico (R${preco_historico_val:.2f}). Sem notifica√ß√£o.")
                                    continue
                                else:
                                    item_logger.info(f"ASIN {asin}: Novo pre√ßo (R${price:.2f}) melhor que hist√≥rico (R${preco_historico_val if preco_historico_val else 'N/A'}).")
                            else:
                                item_logger.info(f"ASIN {asin} n√£o est√° no hist√≥rico. Novo produto 'usado' qualificado.")
                            
                            history[asin] = produto
                            save_history_geral(history)
                        
                        total_produtos_usados_qualificados.append(produto)
                        produtos_processados_e_notificados_na_pagina += 1
                        item_logger.info(f"PRODUTO QUALIFICADO: '{nome}' | Pre√ßo: R${price:.2f} | ASIN: {asin}")

                        if bot_instance_global and TELEGRAM_CHAT_IDS_LIST:
                            message = (
                                f"*{escape_md(nome_fluxo)}*\n\n"
                                f"üì¶ *{escape_md(nome)}*\n"
                                f"üíµ Pre√ßo Usado: *R${price:.2f}*\n"
                                f"üîó [Ver na Amazon]({link})\n\n"
                                f"üè∑Ô∏è ASIN: `{escape_md(asin)}`\n"
                                f"üïí {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}"
                            )
                            for chat_id in TELEGRAM_CHAT_IDS_LIST:
                                await send_telegram_message_async(
                                    bot_instance_global, chat_id, message, ParseMode.MARKDOWN_V2, item_logger
                                )
                    
                    except StaleElementReferenceException:
                        item_logger.warning("Elemento Selenium tornou-se obsoleto. Tentando buscar itens novamente na p√°gina.")
                        break 
                    except Exception as e_item_proc:
                        item_logger.error(f"Erro inesperado ao processar bloco de item {idx}: {e_item_proc}", exc_info=True)
                        continue

                if produtos_processados_e_notificados_na_pagina > 0:
                    logger.info(f"P√°gina {pagina_atual}: {produtos_processados_e_notificados_na_pagina} produtos 'usados' qualificados, processados e notificados.")
                else:
                    logger.info(f"P√°gina {pagina_atual}: Nenhum produto novo ou com pre√ßo melhorado encontrado para notifica√ß√£o (ap√≥s todas as verifica√ß√µes).")
                
                page_processed_successfully = True
                break 

            except WebDriverException as e_wd:
                logger.error(f"Erro de WebDriver ao carregar p√°gina {pagina_atual} (Tentativa {tentativa}): {str(e_wd)[:200]}", exc_info=False)
                if tentativa < max_tentativas_pagina:
                    await asyncio.sleep(random.uniform(15, 30))
                    continue
                else:
                    logger.error(f"Falha cr√≠tica ap√≥s {max_tentativas_pagina} tentativas na p√°gina {pagina_atual} (WebDriverException). Interrompendo {nome_fluxo}.")
                    return total_produtos_usados_qualificados
            except Exception as e_page:
                logger.error(f"Erro geral ao processar p√°gina {pagina_atual} (Tentativa {tentativa}): {e_page}", exc_info=True)
                if tentativa < max_tentativas_pagina:
                    await asyncio.sleep(random.uniform(10, 20))
                    continue
                else:
                    logger.error(f"Falha cr√≠tica ap√≥s {max_tentativas_pagina} tentativas na p√°gina {pagina_atual} (Erro Geral). Interrompendo {nome_fluxo}.")
                    return total_produtos_usados_qualificados
        
        if not page_processed_successfully:
            logger.error(f"N√£o foi poss√≠vel processar a p√°gina {pagina_atual} de {nome_fluxo} ap√≥s {max_tentativas_pagina} tentativas. Abortando este fluxo.")
            return total_produtos_usados_qualificados

        pagina_atual += 1
        if pagina_atual <= max_paginas : 
             await asyncio.sleep(random.uniform(5, 10)) 

    logger.info(
        f"--- Conclu√≠do Fluxo: {nome_fluxo}. M√°ximo de p√°ginas ({max_paginas}) atingido ou fim da pagina√ß√£o. "
        f"Total de produtos 'usados' qualificados encontrados nesta execu√ß√£o: {len(total_produtos_usados_qualificados)} ---"
    )
    return total_produtos_usados_qualificados

async def run_usados_geral_scraper_async():
    logger.info(f"--- [SCRAPER IN√çCIO] Fluxo: {NOME_FLUXO_GERAL} ---")
    driver = None
    try:
        logger.info("Tentando iniciar o driver Selenium...")
        driver = iniciar_driver_sync_worker(logger) 
        if not driver:
            logger.error("Falha cr√≠tica ao iniciar o WebDriver. Abortando scraper.")
            return

        logger.info("Driver Selenium iniciado com sucesso.")
        await get_initial_cookies(driver, logger)
        
        history = {}
        if USAR_HISTORICO:
            history = load_history_geral()
        
        await process_used_products_geral_async(driver, URL_GERAL_USADOS_BASE, NOME_FLUXO_GERAL, history, logger, MAX_PAGINAS_POR_LINK_GLOBAL)
        logger.info("Processamento do fluxo de usados geral conclu√≠do.")

    except Exception as e:
        logger.error(f"Erro catastr√≥fico no fluxo geral de usados (run_usados_geral_scraper_async): {e}", exc_info=True)
    finally:
        if driver:
            logger.info("Tentando fechar o driver Selenium...")
            try:
                driver.quit()
                logger.info("Driver Selenium fechado.")
            except Exception as e_quit:
                logger.error(f"Erro ao fechar o driver: {e_quit}", exc_info=True)
        logger.info(f"--- [SCRAPER FIM] Fluxo: {NOME_FLUXO_GERAL} ---")

def load_proxy_list():
    proxy_list = []
    proxy_hosts = os.getenv("PROXY_HOST", "").strip().split(',')
    proxy_ports = os.getenv("PROXY_PORT", "").strip().split(',')
    proxy_usernames = os.getenv("PROXY_USERNAME", "").strip().split(',')
    proxy_passwords = os.getenv("PROXY_PASSWORD", "").strip().split(',')
    for i in range(min(len(proxy_hosts), len(proxy_ports))):
        host = proxy_hosts[i].strip()
        port = proxy_ports[i].strip()
        username = proxy_usernames[i].strip() if i < len(proxy_usernames) and proxy_usernames[i].strip() else None
        password = proxy_passwords[i].strip() if i < len(proxy_passwords) and proxy_passwords[i].strip() else None
        if host and port: 
            if not host.startswith("http"): 
                proxy_url = f'http://{username}:{password}@{host}:{port}' if username and password else f'http://{host}:{port}'
                proxy_list.append(proxy_url)
            else: 
                 proxy_list.append(host) 
    
    if not proxy_list:
        logger.warning("Nenhum proxy configurado.")
    else:
        logger.info(f"Carregados {len(proxy_list)} proxies.")
    return proxy_list

def test_proxy(proxy_url, logger_param):
    logger_param.info(f"Testando proxy: {proxy_url}")
    try:
        ua_test = UserAgent()
        headers_test = {'User-Agent': ua_test.random}
        response = requests.get("https://www.amazon.com.br", proxies={"http": proxy_url, "https": proxy_url}, timeout=10, headers=headers_test)
        if response.status_code == 200:
            logger_param.info(f"Proxy {proxy_url} testado com sucesso: Status 200")
            return True
        else:
            logger_param.warning(f"Proxy {proxy_url} retornou status inesperado: {response.status_code}")
            return False
    except requests.RequestException as e:
        logger_param.error(f"Erro ao testar proxy {proxy_url}: {e}")
        return False

def get_working_proxy(proxy_list, logger_param):
    if not proxy_list: 
        logger_param.warning("Lista de proxies vazia. Nenhum proxy para testar.")
        return None
    for proxy_url in proxy_list:
        if test_proxy(proxy_url, logger_param):
            return proxy_url
    logger_param.warning("Nenhum proxy funcional encontrado na lista. Prosseguindo sem proxy.")
    return None

def iniciar_driver_sync_worker(current_run_logger, driver_path=None): 
    current_run_logger.info("Iniciando configura√ß√£o do WebDriver...")
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    
    ua = UserAgent()
    user_agent = ua.random
    chrome_options.add_argument(f"user-agent={user_agent}")
    current_run_logger.info(f"User-Agent: {user_agent}")
    
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-popup-blocking"); chrome_options.add_argument("--mute-audio")
    chrome_options.add_argument("--no-first-run"); chrome_options.add_argument("--disable-webgl"); chrome_options.add_argument("--disable-webrtc")
    chrome_options.add_argument("--disable-features=WebRtcHideLocalIpsWithMdns,PrivacySandboxSettings4,OptimizationHints,InterestGroupStorage")
    chrome_options.add_argument("--lang=pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7")
    
    proxies_available = load_proxy_list()
    working_proxy_url = get_working_proxy(proxies_available, current_run_logger) if proxies_available else None
    proxy_actually_configured = False

    if working_proxy_url:
        current_run_logger.info(f"Configurando proxy para Selenium: {working_proxy_url}")
        chrome_options.add_argument(f'--proxy-server={working_proxy_url}') 
        proxy_actually_configured = True
    else:
        current_run_logger.warning("Nenhum proxy funcional. WebDriver iniciar√° sem proxy.")
    current_run_logger.info(f"Op√ß√µes do Chrome: {chrome_options.arguments}")

    service = None; driver = None
    page_load_timeout_val = 120
    try:
        path_from_manager = ChromeDriverManager().install()
        service = Service(path_from_manager)
        current_run_logger.info(f"ChromeDriver via Manager: {path_from_manager}")
        
        driver = webdriver.Chrome(service=service, options=chrome_options)
        current_run_logger.info("WebDriver instanciado.")
        driver.set_page_load_timeout(page_load_timeout_val)
        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {'source': "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"})
        return driver
    except WebDriverException as e_wd_init:
        if ("ERR_NO_SUPPORTED_PROXIES" in str(e_wd_init) or "ERR_PROXY_CONNECTION_FAILED" in str(e_wd_init)) and proxy_actually_configured:
            current_run_logger.error(f"Erro de proxy ({working_proxy_url}) ao iniciar WebDriver: {str(e_wd_init)}. Tentando sem proxy.")
            chrome_options.arguments = [arg for arg in chrome_options.arguments if not arg.startswith('--proxy-server')]
            try:
                driver = webdriver.Chrome(service=service, options=chrome_options) 
                current_run_logger.info("WebDriver instanciado sem proxy ap√≥s falha inicial com proxy.")
                driver.set_page_load_timeout(page_load_timeout_val)
                driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {'source': "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"})
                return driver
            except Exception as e_retry_no_proxy:
                current_run_logger.error(f"Falha ao tentar iniciar WebDriver sem proxy ap√≥s erro de proxy: {e_retry_no_proxy}", exc_info=True)
                if driver: driver.quit()
                raise
        else:
            current_run_logger.error(f"WebDriverException n√£o relacionada a proxy configurado ao iniciar WebDriver: {e_wd_init}", exc_info=True)
            if driver: driver.quit()
            raise
    except Exception as e_init:
        current_run_logger.error(f"Erro geral ao iniciar WebDriver: {e_init}", exc_info=True)
        if driver: driver.quit()
        raise

async def get_initial_cookies(driver, logger_param):
    logger_param.info("Acessando p√°gina inicial para obter cookies...")
    try:
        await asyncio.to_thread(driver.get, "https://www.amazon.com.br")
        await asyncio.sleep(random.uniform(3, 5))
        await asyncio.to_thread(wait_for_page_load, driver, logger_param)
        logger_param.info("Cookies iniciais obtidos.")
    except Exception as e:
        logger_param.error(f"Erro ao obter cookies iniciais: {e}", exc_info=True)

async def simulate_scroll(driver, logger_param):
    logger_param.debug("Simulando rolagem na p√°gina...")
    try:
        await asyncio.to_thread(driver.execute_script, "window.scrollTo(0, document.body.scrollHeight*0.6);")
        await asyncio.sleep(random.uniform(1, 2))
        await asyncio.to_thread(driver.execute_script, "window.scrollTo(0, document.body.scrollHeight);") 
        await asyncio.sleep(random.uniform(0.5, 1.5))
        await asyncio.to_thread(driver.execute_script, "window.scrollTo(0, 0);") 
        logger_param.debug("Rolagem simulada com sucesso.")
    except Exception as e:
        logger_param.error(f"Erro ao simular rolagem: {e}", exc_info=True)

async def send_telegram_message_async(bot, chat_id, message, parse_mode, msg_logger):
    msg_logger.debug(f"Tentando enviar mensagem para chat_id: {chat_id}")
    if not bot:
        msg_logger.error(f"[{msg_logger.name}] Inst√¢ncia do Bot n√£o fornecida.")
        return False
    try:
        await bot.send_message(chat_id=chat_id, text=message, parse_mode=parse_mode)
        msg_logger.info(f"[{msg_logger.name}] Notifica√ß√£o Telegram enviada para CHAT_ID {chat_id}.")
        return True
    except TelegramError as e_tg:
        msg_logger.error(f"[{msg_logger.name}] Erro Telegram ao enviar para CHAT_ID {chat_id}: {e_tg.message}", exc_info=False)
        return False
    except Exception as e_msg:
        msg_logger.error(f"[{msg_logger.name}] Erro inesperado ao enviar msg para CHAT_ID {chat_id}: {e_msg}", exc_info=True)
        return False

def escape_md(text):
    return re.sub(r'([_\*\[\]\(\)~`>#+\-=|{}.!])', r'\\\1', str(text))

def load_history_geral():
    history_path = os.path.join(HISTORY_DIR_BASE, HISTORY_FILENAME_USADOS_GERAL)
    logger.info(f"Carregando hist√≥rico de: {history_path}")
    if os.path.exists(history_path):
        try:
            with open(history_path, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
            logger.info(f"Hist√≥rico carregado: {len(history_data)} ASINs.")
            return history_data
        except Exception as e:
            logger.error(f"Erro ao carregar/decodificar hist√≥rico de '{history_path}': {e}. Retornando vazio.", exc_info=True)
            return {}
    else:
        logger.info("Arquivo de hist√≥rico n√£o encontrado. Retornando hist√≥rico vazio.")
        return {}

def save_history_geral(history):
    history_path = os.path.join(HISTORY_DIR_BASE, HISTORY_FILENAME_USADOS_GERAL)
    logger.info(f"Salvando hist√≥rico ({len(history)} ASINs) em: {history_path}")
    try:
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        logger.info("Hist√≥rico salvo com sucesso.")
    except Exception as e:
        logger.error(f"Erro ao salvar hist√≥rico em '{history_path}': {e}", exc_info=True)

def get_url_for_page_worker(base_url, page_number, current_run_logger):
    current_run_logger.debug(f"Gerando URL para p√°gina {page_number} a partir de base: {base_url}")
    parsed_url = urlparse(base_url)
    query_params = parse_qs(parsed_url.query)
    query_params['page'] = [str(page_number)]
    qid_time = int(time.time() * 1000)
    query_params['qid'] = [str(qid_time)]
    query_params['ref'] = [f'sr_pg_{page_number}']
    new_query = urlencode(query_params, doseq=True)
    final_url = urlunparse(parsed_url._replace(query=new_query))
    current_run_logger.debug(f"URL da p√°gina gerada: {final_url}")
    return final_url

def check_captcha_sync_worker(driver, current_run_logger):
    current_run_logger.debug("Verificando a presen√ßa de CAPTCHA.")
    try:
        WebDriverWait(driver, 3).until(EC.any_of( 
            EC.presence_of_element_located((By.CSS_SELECTOR, "form[action*='captcha'] img")),
            EC.presence_of_element_located((By.XPATH, "//h4[contains(text(), 'Insira os caracteres')]")),
            EC.presence_of_element_located((By.XPATH, "//h4[contains(text(), 'Digite os caracteres que voc√™ v√™ abaixo')]")),
            EC.presence_of_element_located((By.CSS_SELECTOR, "iframe[src*='captcha']"))
        ))
        current_run_logger.warning(f"CAPTCHA detectado! URL: {driver.current_url}")
        timestamp_captcha = datetime.now().strftime('%Y%m%d_%H%M%S')
        screenshot_path = os.path.join(DEBUG_LOGS_DIR_BASE, f"captcha_usados_geral_{timestamp_captcha}.png")
        html_path = os.path.join(DEBUG_LOGS_DIR_BASE, f"captcha_usados_geral_{timestamp_captcha}.html")
        try:
            driver.save_screenshot(screenshot_path)
            current_run_logger.info(f"Screenshot do CAPTCHA salvo em: {screenshot_path}")
            with open(html_path, "w", encoding="utf-8") as f_html:
                f_html.write(driver.page_source)
            current_run_logger.info(f"HTML do CAPTCHA salvo em: {html_path}")
        except Exception as e_save_captcha:
            current_run_logger.error(f"Erro ao salvar debug do CAPTCHA: {e_save_captcha}")
        return True
    except (TimeoutException, NoSuchElementException):
        current_run_logger.debug("Nenhum CAPTCHA detectado (ou timeout curto).")
        return False
    except Exception as e_check_captcha:
        current_run_logger.error(f"Erro inesperado ao verificar CAPTCHA: {e_check_captcha}", exc_info=True)
        return False

def check_amazon_error_page_sync_worker(driver, current_run_logger):
    current_run_logger.debug("Verificando se √© p√°gina de erro da Amazon.")
    error_page_detected = False
    page_title_lower = ""
    try:
        page_title_lower = driver.title.lower()
        error_title_keywords = ["desculpe", "algo deu errado", "sorry", "problema", "servi√ßo indispon√≠vel", "error", "n√£o encontrada"]
        if any(keyword in page_title_lower for keyword in error_title_keywords):
            current_run_logger.warning(f"P√°gina de erro detectada pelo t√≠tulo: {driver.title}")
            error_page_detected = True
        
        error_selectors_check = [
            (By.XPATH, "//img[contains(@alt, 'Desculpe') or contains(@alt, 'Sorry')]"), 
            (By.XPATH, "//*[contains(text(), 'Algo deu errado')]"),
            (By.XPATH, "//*[contains(text(), 'Desculpe-nos')]"),
            (By.XPATH, "//*[contains(text(), 'Servi√ßo Indispon√≠vel')]"),
            (By.CSS_SELECTOR, "div#g"), 
        ]
        if not error_page_detected: 
            for by, selector in error_selectors_check:
                try:
                    element = driver.find_element(by, selector)
                    current_run_logger.warning(f"P√°gina de erro detectada por elemento: {selector} | Texto (se houver): {element.text[:100] if element.text else 'N/A'}")
                    error_page_detected = True
                    break 
                except NoSuchElementException:
                    continue
                except StaleElementReferenceException:
                     current_run_logger.warning(f"Elemento {selector} ficou obsoleto ao checar p√°gina de erro.")
                     continue
        
        if not error_page_detected:
            try:
                driver.find_element(By.CSS_SELECTOR, SELETOR_RESULTADOS_CONT) 
                current_run_logger.debug("Cont√™iner de resultados encontrado. Aparentemente n√£o √© p√°gina de erro.")
            except NoSuchElementException:
                current_run_logger.warning(f"Cont√™iner de resultados '{SELETOR_RESULTADOS_CONT}' N√ÉO encontrado. Pode ser p√°gina de resultados vazia ou erro sutil.")
        
        return error_page_detected

    except Exception as e:
        current_run_logger.error(f"Erro ao verificar p√°gina de erro da Amazon: {e}", exc_info=True)
        return True 
    finally:
        if error_page_detected and driver.current_url:
            timestamp_error = datetime.now().strftime('%Y%m%d_%H%M%S')
            screenshot_path = os.path.join(DEBUG_LOGS_DIR_BASE, f"amazon_error_page_{timestamp_error}.png")
            html_path = os.path.join(DEBUG_LOGS_DIR_BASE, f"amazon_error_page_{timestamp_error}.html")
            try:
                driver.save_screenshot(screenshot_path)
                current_run_logger.info(f"Screenshot da p√°gina de erro salvo em: {screenshot_path}")
                with open(html_path, "w", encoding="utf-8") as f_html_err: 
                    f_html_err.write(driver.page_source)
                current_run_logger.info(f"HTML da p√°gina de erro salvo em: {html_path}")
            except Exception as e_save_err: 
                current_run_logger.error(f"Erro ao salvar debug da p√°gina de erro: {e_save_err}")

def wait_for_page_load(driver, logger_param, timeout=60):
    logger_param.debug(f"Aguardando carregamento completo da p√°gina (timeout={timeout}s)...")
    try:
        WebDriverWait(driver, timeout).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        logger_param.info("P√°gina carregada (document.readyState is 'complete').")
    except TimeoutException:
        logger_param.warning("Timeout ao esperar carregamento completo da p√°gina.")
    except Exception as e:
        logger_param.error(f"Erro ao esperar carregamento da p√°gina: {e}", exc_info=True)

if __name__ == "__main__":
    current_max_pages_env = os.getenv("MAX_PAGINAS_USADOS_GERAL")
    if current_max_pages_env:
        try:
            MAX_PAGINAS_POR_LINK_GLOBAL = int(current_max_pages_env)
            logger.info(f"MAX_PAGINAS_POR_LINK_GLOBAL atualizado para: {MAX_PAGINAS_POR_LINK_GLOBAL} (via env var no __main__)")
        except ValueError:
            logger.warning(f"Valor inv√°lido para MAX_PAGINAS_USADOS_GERAL no __main__: '{current_max_pages_env}'. Usando o valor inicial: {MAX_PAGINAS_POR_LINK_GLOBAL}")
    
    asyncio.run(run_usados_geral_scraper_async())
